# Job Scraper (Profesia.sk) ğŸ¤–

<div align="center">
  <img src="https://lh3.googleusercontent.com/a/ACg8ocKktHJlvkQK85EHrR8Sm1zUzwG0ZeF_AGPdBvFW9Pd0CPYITSjn=s288-c-no" alt="ITSsafer Logo" width="150"/>
</div>

[![Node.js Version](https://img.shields.io/badge/node-%3E%3D18.0.0-brightgreen.svg)](https://nodejs.org/)
[![License](https://img.shields.io/badge/license-AGPL--3.0-blue.svg)](LICENSE)
[![Commercial License](https://img.shields.io/badge/license-Commercial-green.svg)](LICENSE.commercial)
[![Code Style](https://img.shields.io/badge/code%20style-eslint-brightgreen.svg)](https://eslint.org)


InteligentnÃ½ crawler na extrahovanie pracovnÃ½ch ponÃºk z portÃ¡lu Profesia.sk, vytvorenÃ½ s Apify SDK a Crawlee technolÃ³giami. Projekt implementuje sofistikovanÃ© zÃ­skavanie dÃ¡t s reÅ¡pektovanÃ­m zÃ¡sad etickÃ©ho web scrapingu.

## ğŸ¯ O Projekte

Tento nÃ¡stroj je navrhnutÃ½ ako sofistikovanÃ© rieÅ¡enie pre automatizovanÃ© zbieranie pracovnÃ½ch ponÃºk z Profesia.sk. VyuÅ¾Ã­va modernÃ© technolÃ³gie a osvedÄenÃ© postupy pre etickÃ½ web scraping:

### KÄ¾ÃºÄovÃ© Funkcie

- **InteligentnÃ½ Crawling**: ReÅ¡pektuje robots.txt a implementuje pokroÄilÃ© rate limiting
- **RobustnÃ© Spracovanie**: SpoÄ¾ahlivÃ¡ extrakcia a normalizÃ¡cia dÃ¡t
- **FlexibilnÃ¡ KonfigurÃ¡cia**: Ä½ahkÃ© prispÃ´sobenie cez .env sÃºbor
- **KvalitnÃ½ VÃ½stup**: Å truktÃºrovanÃ© dÃ¡ta v JSON a CSV formÃ¡toch

### TechnickÃ© Detaily

- VytvorenÃ© s Node.js â‰¥ 18.0.0
- VyuÅ¾Ã­va Apify SDK a Crawlee framework
- Implementuje pokroÄilÃ© retry stratÃ©gie
- ZabezpeÄuje validÃ¡ciu a deduplikÃ¡ciu dÃ¡t

## ğŸ“Š ArchitektÃºra

```mermaid
graph TD
    subgraph Jadro ["Jadro AplikÃ¡cie"]
        A[main.js<br/>VstupnÃ½ Bod] --> B[ProfesiaAdapter]
        B --> C[PuppeteerCrawler]
    end
    
    subgraph Crawling ["ZÃ­skavanie DÃ¡t"]
        C --> D[Listing StrÃ¡nky<br/>PrehÄ¾ad PozÃ­ciÃ­]
        C --> E[Detail StrÃ¡nky<br/>KompletnÃ© Info]
        D --> |Odkazy na<br/>ÄalÅ¡ie StrÃ¡nky| D
    end
    
    subgraph Spracovanie ["Spracovanie DÃ¡t"]
        E --> F[jobDetailParser<br/>HTML Extrakcia]
        F --> G[ValidÃ¡cia DÃ¡t<br/>Kontrola Kvality]
        G --> H[DeduplikÃ¡cia<br/>UnikÃ¡tne ZÃ¡znamy]
    end
    
    subgraph Export ["Export DÃ¡t"]
        H --> I[JSON Export<br/>SurovÃ© DÃ¡ta]
        H --> J[CSV Export<br/>Analytika]
        I --> K[ÃšloÅ¾isko]
        J --> K[ÃšloÅ¾isko]
    end
    
    style A fill:#f9f,stroke:#333,stroke-width:4px
    style B fill:#bbf,stroke:#333,stroke-width:2px
    style F fill:#bbf,stroke:#333,stroke-width:2px
    
    classDef default fill:#f9f9f9,stroke:#333,stroke-width:1px
    classDef core fill:#e1f5fe,stroke:#0277bd,stroke-width:2px
    classDef process fill:#f3e5f5,stroke:#8e24aa,stroke-width:2px
    classDef export fill:#e8f5e9,stroke:#2e7d32,stroke-width:2px
    
    class A,B,C core
    class D,E,F,G,H process
    class I,J,K export
```

### SekvenÄnÃ½ Diagram

```mermaid
sequenceDiagram
    participant M as Main
    participant A as ProfesiaAdapter
    participant R as RobotsChecker
    participant C as Crawler
    participant P as Parser
    participant V as Validator
    participant E as Exporter
    
    rect rgb(240, 240, 240)
        Note right of M: InicializaÄnÃ¡ FÃ¡za
        M->>A: InicializÃ¡cia()
        A->>R: Kontrola Robots.txt
        R-->>A: PovolenÃ©/BlokovanÃ©
    end
    
    rect rgb(245, 245, 245)
        Note right of A: FÃ¡za Crawlingu
        A->>C: SpustiÅ¥Crawling()
        activate C
        loop Pre KaÅ¾dÃº Listing StrÃ¡nku
            C->>C: Spracovanie Listingu
            C->>C: Extrakcia Odkazov
            loop Pre KaÅ¾dÃº PozÃ­ciu
                C->>P: Parsovanie Detailu
                activate P
                P->>P: Extrakcia DÃ¡t
                P->>P: NormalizÃ¡cia PolÃ­
                P-->>C: DÃ¡ta PozÃ­cie
                deactivate P
            end
        end
        deactivate C
    end
    
    rect rgb(250, 250, 250)
        Note right of C: FÃ¡za Spracovania
        C->>V: ValidÃ¡cia DÃ¡t
        activate V
        V->>V: Kontrola Kvality
        V->>V: DeduplikÃ¡cia
        V-->>A: ValidnÃ© PozÃ­cie
        deactivate V
    end
    
    rect rgb(255, 255, 255)
        Note right of A: FÃ¡za Exportu
        A->>E: Export DÃ¡t
        activate E
        E->>E: FormÃ¡tovanie JSON
        E->>E: FormÃ¡tovanie CSV
        E-->>M: Export DokonÄenÃ½
        deactivate E
    end
```

### ğŸ”„ Proces Spracovania

1. **InicializÃ¡cia** (`main.js`):
   - NaÄÃ­tanie konfigurÃ¡cie z `.env`
   - Vytvorenie inÅ¡tancie `ProfesiaAdapter`

2. **Crawling** (`ProfesiaAdapter`):
   - Overenie robots.txt
   - Riadenie rate-limiting a sÃºbeÅ¾nÃ½ch requestov
   - InteligentnÃ© retry/backoff stratÃ©gie

3. **Parsovanie** (`jobDetailParser`):
   - Extrakcia Å¡truktÃºrovanÃ½ch dÃ¡t
   - NormalizÃ¡cia dÃ¡tumov a platov
   - ValidÃ¡cia Ãºplnosti

4. **Post-processing**:
   - DeduplikÃ¡cia podÄ¾a jobId
   - ValidÃ¡cia kvality dÃ¡t
   - Export do JSON/CSV

## ğŸš€ RÃ½chly Å tart

### PoÅ¾iadavky

- Node.js â‰¥ 18.0.0
- npm (node package manager)

### InÅ¡talÃ¡cia

```bash
# Klonovanie repozitÃ¡ra
git clone <repo-url>
cd job-scraper-itssafer

# InÅ¡talÃ¡cia zÃ¡vislostÃ­
npm install
```

### KonfigurÃ¡cia

Vytvorte `.env` sÃºbor v root adresÃ¡ri:

```env
# Base URL pre crawling
START_URL=https://www.profesia.sk/praca/

# Limity a konfigurÃ¡cia
CONCURRENCY=3
MAX_PAGES=10
DELAY_MIN=1000
DELAY_MAX=1500
```

### Spustenie

```bash
npm start
```

### Spustenie s Podman

Projekt je plne kontajnerizovanÃ½ a mÃ´Å¾e byÅ¥ spustenÃ½ pomocou Podman:

```bash
# Zostavenie kontajnera
podman build -t job-scraper-itssafer .

# Spustenie kontajnera
podman run -d \
  --name job-scraper \
  -v "./output:/app/output" \
  -v "./.env:/app/.env" \
  job-scraper-itssafer

# Kontrola logov kontajnera
podman logs -f job-scraper

# Zastavenie kontajnera
podman stop job-scraper
```

VÃ½hody pouÅ¾itia Podman:
- Rootless kontajnery pre lepÅ¡iu bezpeÄnosÅ¥
- OCI kompatibilita
- NevyÅ¾aduje daemon
- NatÃ­vna systemd integrÃ¡cia
- MultiplatformovÃ¡ podpora

## ğŸ“‹ DÃ¡tovÃ½ Model

### Å truktÃºra VÃ½stupu

```typescript
interface Job {
    jobTitle: string;
    companyName: string;
    location: string;
    salary: {
        min: number | null;
        max: number | null;
        currency: string | null;
        period: string | null;
    };
    employmentType: string;
    seniority: string | null;
    tags: string[];
    postedAt: string; // ISO dÃ¡tum
    jobId: string;
    jobUrl: string;
    companyUrl: string;
    description: string;
}
```

### PrÃ­klad VÃ½stupu

```json
{
    "jobTitle": "Senior Software Engineer",
    "companyName": "Example Corp",
    "location": "Bratislava",
    "salary": {
        "min": 3000,
        "max": 5000,
        "currency": "EUR",
        "period": "mesiac"
    },
    "employmentType": "plnÃ½ ÃºvÃ¤zok",
    "seniority": "senior",
    "tags": ["java", "spring", "postgresql"],
    "postedAt": "2025-10-07T00:00:00.000Z",
    "jobId": "5162007",
    "jobUrl": "https://www.profesia.sk/praca/example-corp/O5162007",
    "companyUrl": "https://firma.profesia.sk/example-corp",
    "description": "HÄ¾adÃ¡me skÃºsenÃ©ho software inÅ¾iniera..."
}
```

## ğŸ›  TechnickÃ¡ ImplementÃ¡cia

### Komponenty

1. **ProfesiaAdapter** (`src/adapters/ProfesiaAdapter.js`)
   - Implementuje crawling logiku
   - Spravuje session a requesty
   - ZabezpeÄuje dodrÅ¾iavanie robots.txt

2. **JobDetailParser** (`src/parsers/jobDetailParser.js`)
   - Parsuje HTML detail strÃ¡nky
   - Normalizuje dÃ¡tumy a platy
   - Extraktuje Å¡truktÃºrovanÃ© dÃ¡ta

3. **Core Utils** (`src/utils/core.js`)
   - ValidÃ¡cia a deduplikÃ¡cia
   - Logovanie a monitoring
   - PomocnÃ© funkcie

### SekvenÄnÃ½ Diagram

```mermaid
sequenceDiagram
    participant M as Main
    participant A as Adapter
    participant C as Crawler
    participant P as Parser
    participant E as Exporter

    M->>A: crawl()
    A->>A: checkRobots()
    A->>C: run()
    C->>C: processListingPage
    C->>C: enqueueDetailPages
    C->>P: parseJobDetail()
    P->>P: normalize()
    P-->>C: jobData
    C-->>A: allJobs
    A->>A: validate()
    A->>A: dedupe()
    A->>E: export()
    E-->>M: done
```

## ğŸ§ª Testovanie

Projekt zahÅ•Åˆa unit testy a integraÄnÃ© testy:

```bash
# Spustenie unit testov pre parser
node tests/parser_unit.test.js

# Spustenie integraÄnÃ©ho testu adaptÃ©ra
node tests/run_adapter_check.js
```

### Pokrytie Testov

- **Parser Testy**: ValidÃ¡cia spracovania platov a dÃ¡tumov
- **Adapter Testy**: Smoke testy pre crawling logiku
- **IntegrÃ¡cia**: End-to-end validÃ¡cia procesu

## ğŸ“ˆ Monitoring & Reportovanie

Crawler generuje detailnÃ© logy a Å¡tatistiky:

- PoÄet spracovanÃ½ch strÃ¡nok
- ÃšspeÅ¡nosÅ¥/neÃºspeÅ¡nosÅ¥
- ÄŒasovÃ© metriky
- VyuÅ¾itie pamÃ¤te

### SÃºhrnnÃ½ Report

Po ukonÄenÃ­ crawlingu sa generuje report s agregovanÃ½mi dÃ¡tami:

- Top lokality
- DistribÃºcia platov
- NajÄastejÅ¡ie tagy/zruÄnosti
- Å tatistiky spracovania

## âš–ï¸ EtickÃ½ Scraping

Projekt implementuje osvedÄenÃ© postupy pre etickÃ½ web scraping:

- **Robots.txt**: AutomatickÃ¡ kontrola a dodrÅ¾iavanie
- **Rate Limiting**: KonfigurovateÄ¾nÃ© delays medzi requestami
- **SÃºbeÅ¾nÃ½ PrÃ­stup**: LimitovanÃ© paralelnÃ© requesty
- **User-Agent**: TransparentnÃ½ identifikÃ¡tor
- **Error Handling**: Graceful degradation pri chybÃ¡ch

## ğŸ” PoznÃ¡mky k ImplementÃ¡cii

### Rate Limiting

Crawler pouÅ¾Ã­va sofistikovanÃ½ rate limiting:

```javascript
// PrÃ­klad implementÃ¡cie v ProfesiaAdapter
function randomDelay(min, max) {
    return Math.floor(Math.random() * (max - min + 1)) + min;
}

// PouÅ¾itie s env konfigurÃ¡ciou
await sleep(randomDelay(this.delayMin, this.delayMax));
```

### Retry StratÃ©gia

ImplementovanÃ¡ exponenciÃ¡lna backoff stratÃ©gia:

```javascript
const crawler = new PuppeteerCrawler({
    maxRequestRetries: 3,
    // Crawlee handles exponential backoff internally
});
```

## ğŸ“¦ VÃ½stupy

Crawler generuje dva typy vÃ½stupov:

1. **JSON** (`./output/jobs.json`):
   - KompletnÃ© dÃ¡ta v Å¡truktÃºrovanom formÃ¡te
   - VhodnÃ© pre ÄalÅ¡ie spracovanie

2. **CSV** (`./output/jobs.csv`):
   - TabuÄ¾kovÃ½ formÃ¡t pre analÃ½zu
   - KompatibilnÃ© s Excel/Google Sheets

## ğŸ¤ Prispievanie

1. Fork repozitÃ¡ra
2. Vytvorte feature branch
3. Commit zmeny
4. Push do branch
5. Vytvorte Pull Request

## ğŸ“„ Licencovanie

Tento projekt pouÅ¾Ã­va duÃ¡lne licencovanie:

### Open-Source Licencia (AGPL-3.0)
- VoÄ¾nÃ© pouÅ¾itie pre nekomerÄnÃ© ÃºÄely
- ZdrojovÃ½ kÃ³d musÃ­ zostaÅ¥ otvorenÃ½
- DerivÃ¡tovÃ© diela musia byÅ¥ tieÅ¾ pod AGPL-3.0
- VzÅ¥ahuje sa aj na sieÅ¥ovÃ©/API pouÅ¾itie

### KomerÄnÃ¡ Licencia
Pre komerÄnÃ© pouÅ¾itie je potrebnÃ¡ komerÄnÃ¡ licencia, ktorÃ¡ poskytuje:
- MoÅ¾nosÅ¥ pouÅ¾itia kÃ³du v uzavretÃ½ch projektoch
- Bez povinnosti publikovania zdrojovÃ©ho kÃ³du
- PrioritnÃº podporu
- VlastnÃ© Ãºpravy bez zdieÄ¾ania

Pre zÃ­skanie komerÄnej licencie kontaktujte:
- Email: [itssafer@itssafer.org](mailto:itssafer@itssafer.org)
- Predmet: Job Scraper - Commercial License

## ğŸ™ PoÄakovanie

- [Apify SDK](https://sdk.apify.com/)
- [Crawlee](https://crawlee.dev/)
- [Puppeteer](https://pptr.dev/)

## ğŸ‘¤ Autor

- **Meno**: KristiÃ¡n KaÅ¡nÃ­k
- **Email**: [itssafer@itssafer.org](mailto:itssafer@itssafer.org)
- **GitHub**: [ITSsafer-DevOps](https://github.com/ITSsafer-DevOps)

---

VytvorenÃ© s â¤ï¸ pre lepÅ¡iu analÃ½zu pracovnÃ©ho trhu
