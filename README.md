# Job Scraper (Profesia.sk) ü§ñ

<div align="center">
  <img src="https://lh3.googleusercontent.com/a/ACg8ocKktHJlvkQK85EHrR8Sm1zUzwG0ZeF_AGPdBvFW9Pd0CPYITSjn=s288-c-no" alt="ITSsafer Logo" width="150"/>
</div>

[![Node.js Version](https://img.shields.io/badge/node-%3E%3D18.0.0-brightgreen.svg)](https://nodejs.org/)
[![License](https://img.shields.io/badge/license-AGPL--3.0-blue.svg)](LICENSE)
[![Commercial License](https://img.shields.io/badge/license-Commercial-green.svg)](LICENSE.commercial)
[![Code Style](https://img.shields.io/badge/code%20style-eslint-brightgreen.svg)](https://eslint.org)

[üá∏üá∞ Slovensk√° verzia](README.sk.md)

Intelligent crawler for extracting job listings from Profesia.sk portal, built with Apify SDK and Crawlee technologies. The project implements sophisticated data acquisition with respect to ethical web scraping principles.

## üéØ About the Project

This tool is designed as a sophisticated solution for automated job listing collection from Profesia.sk. It utilizes modern technologies and best practices for ethical web scraping:

### Key Features

- **Intelligent Crawling**: Respects robots.txt and implements advanced rate limiting
- **Robust Processing**: Reliable data extraction and normalization
- **Flexible Configuration**: Easy customization via .env file
- **Quality Output**: Structured data in JSON and CSV formats

### Technical Details

- Built with Node.js ‚â• 18.0.0
- Utilizes Apify SDK and Crawlee framework
- Implements advanced retry strategies
- Ensures data validation and deduplication

## üìä Architecture

```mermaid
graph TD
    subgraph Core ["Core Application"]
        A[main.js<br/>Entry Point] --> B[ProfesiaAdapter]
        B --> C[PuppeteerCrawler]
    end
    
    subgraph Crawling ["Data Acquisition"]
        C --> D[Listing Pages<br/>Job Overview]
        C --> E[Detail Pages<br/>Complete Info]
        D --> |Next Page<br/>Links| D
    end
    
    subgraph Processing ["Data Processing"]
        E --> F[jobDetailParser<br/>HTML Extraction]
        F --> G[Data Validation<br/>Quality Check]
        G --> H[Deduplication<br/>Unique Entries]
    end
    
    subgraph Export ["Data Export"]
        H --> I[JSON Export<br/>Raw Data]
        H --> J[CSV Export<br/>Analytics]
        I --> K[Storage]
        J --> K[Storage]
    end
    
    style A fill:#f9f,stroke:#333,stroke-width:4px
    style B fill:#bbf,stroke:#333,stroke-width:2px
    style F fill:#bbf,stroke:#333,stroke-width:2px
    
    classDef default fill:#f9f9f9,stroke:#333,stroke-width:1px
    classDef core fill:#e1f5fe,stroke:#0277bd,stroke-width:2px
    classDef process fill:#f3e5f5,stroke:#8e24aa,stroke-width:2px
    classDef export fill:#e8f5e9,stroke:#2e7d32,stroke-width:2px
    
    class A,B,C core
    class D,E,F,G,H process
    class I,J,K export
```

### Sequence Diagram

```mermaid
sequenceDiagram
    participant M as Main
    participant A as ProfesiaAdapter
    participant R as RobotsChecker
    participant C as Crawler
    participant P as Parser
    participant V as Validator
    participant E as Exporter
    
    rect rgb(240, 240, 240)
        Note right of M: Initialization Phase
        M->>A: Initialize()
        A->>R: Check Robots.txt
        R-->>A: Allowed/Blocked
    end
    
    rect rgb(245, 245, 245)
        Note right of A: Crawling Phase
        A->>C: StartCrawling()
        activate C
        loop For Each Listing Page
            C->>C: Process Listings
            C->>C: Extract Job Links
            loop For Each Job
                C->>P: Parse Job Detail
                activate P
                P->>P: Extract Data
                P->>P: Normalize Fields
                P-->>C: Job Data
                deactivate P
            end
        end
        deactivate C
    end
    
    rect rgb(250, 250, 250)
        Note right of C: Processing Phase
        C->>V: Validate Data
        activate V
        V->>V: Quality Check
        V->>V: Deduplicate
        V-->>A: Valid Jobs
        deactivate V
    end
    
    rect rgb(255, 255, 255)
        Note right of A: Export Phase
        A->>E: Export Data
        activate E
        E->>E: Format JSON
        E->>E: Format CSV
        E-->>M: Export Complete
        deactivate E
    end
```

### üîÑ Processing Flow

1. **Initialization** (`main.js`):
   - Loading configuration from `.env`
   - Creating `ProfesiaAdapter` instance

2. **Crawling** (`ProfesiaAdapter`):
   - Robots.txt verification
   - Rate-limiting and concurrent requests management
   - Intelligent retry/backoff strategies

3. **Parsing** (`jobDetailParser`):
   - Structured data extraction
   - Date and salary normalization
   - Completeness validation

4. **Post-processing**:
   - Deduplication by jobId
   - Data quality validation
   - Export to JSON/CSV

## üöÄ Quick Start

### Prerequisites

- Node.js ‚â• 18.0.0
- npm (node package manager)

### Installation

```bash
# Clone repository
git clone <repo-url>
cd job-scraper-itssafer

# Install dependencies
npm install
```

### Configuration

Create a `.env` file in the root directory:

```env
# Base URL for crawling
START_URL=https://www.profesia.sk/praca/

# Limits and configuration
CONCURRENCY=3
MAX_PAGES=10
DELAY_MIN=1000
DELAY_MAX=1500
```

### Running

```bash
npm start
```

### Running with Podman

The project is fully containerized and can be run using Podman:

```bash
# Build the container
podman build -t job-scraper-itssafer .

# Run the container
podman run -d \
  --name job-scraper \
  -v "./output:/app/output" \
  -v "./.env:/app/.env" \
  job-scraper-itssafer

# Check container logs
podman logs -f job-scraper

# Stop the container
podman stop job-scraper
```

Benefits of using Podman:
- Rootless containers for better security
- OCI compliance
- No daemon requirement
- Native systemd integration
- Cross-platform support

## üìã Data Model

### Output Structure

```typescript
interface Job {
    jobTitle: string;
    companyName: string;
    location: string;
    salary: {
        min: number | null;
        max: number | null;
        currency: string | null;
        period: string | null;
    };
    employmentType: string;
    seniority: string | null;
    tags: string[];
    postedAt: string; // ISO date
    jobId: string;
    jobUrl: string;
    companyUrl: string;
    description: string;
}
```

### Output Example

```json
{
    "jobTitle": "Senior Software Engineer",
    "companyName": "Example Corp",
    "location": "Bratislava",
    "salary": {
        "min": 3000,
        "max": 5000,
        "currency": "EUR",
        "period": "month"
    },
    "employmentType": "full time",
    "seniority": "senior",
    "tags": ["java", "spring", "postgresql"],
    "postedAt": "2025-10-07T00:00:00.000Z",
    "jobId": "5162007",
    "jobUrl": "https://www.profesia.sk/praca/example-corp/O5162007",
    "companyUrl": "https://firma.profesia.sk/example-corp",
    "description": "We are looking for experienced software engineer..."
}
```

## üõ† Technical Implementation

### Components

1. **ProfesiaAdapter** (`src/adapters/ProfesiaAdapter.js`)
   - Implements crawling logic
   - Manages session and requests
   - Ensures robots.txt compliance

2. **JobDetailParser** (`src/parsers/jobDetailParser.js`)
   - Parses HTML detail pages
   - Normalizes dates and salaries
   - Extracts structured data

3. **Core Utils** (`src/utils/core.js`)
   - Validation and deduplication
   - Logging and monitoring
   - Utility functions

### Sequence Diagram

```mermaid
sequenceDiagram
    participant M as Main
    participant A as Adapter
    participant C as Crawler
    participant P as Parser
    participant E as Exporter

    M->>A: crawl()
    A->>A: checkRobots()
    A->>C: run()
    C->>C: processListingPage
    C->>C: enqueueDetailPages
    C->>P: parseJobDetail()
    P->>P: normalize()
    P-->>C: jobData
    C-->>A: allJobs
    A->>A: validate()
    A->>A: dedupe()
    A->>E: export()
    E-->>M: done
```

## üß™ Testing

The project includes unit tests and integration tests:

```bash
# Run unit tests for parser
node tests/parser_unit.test.js

# Run integration test for adapter
node tests/run_adapter_check.js
```

### Test Coverage

- **Parser Tests**: Validation of salary and date processing
- **Adapter Tests**: Smoke tests for crawling logic
- **Integration**: End-to-end flow validation

## üìà Monitoring & Reporting

The crawler generates detailed logs and statistics:

- Pages processed count
- Success/failure rate
- Timing metrics
- Memory usage

### Summary Report

After crawling completion, a report is generated with aggregated data:

- Top locations
- Salary distribution
- Most common tags/skills
- Processing statistics

## ‚öñÔ∏è Ethical Scraping

The project implements best practices for ethical web scraping:

- **Robots.txt**: Automatic checking and compliance
- **Rate Limiting**: Configurable delays between requests
- **Concurrent Access**: Limited parallel requests
- **User-Agent**: Transparent identifier
- **Error Handling**: Graceful degradation on errors

## üîç Implementation Notes

### Rate Limiting

Crawler uses sophisticated rate limiting:

```javascript
// Example implementation in ProfesiaAdapter
function randomDelay(min, max) {
    return Math.floor(Math.random() * (max - min + 1)) + min;
}

// Usage with env configuration
await sleep(randomDelay(this.delayMin, this.delayMax));
```

### Retry Strategy

Implemented exponential backoff strategy:

```javascript
const crawler = new PuppeteerCrawler({
    maxRequestRetries: 3,
    // Crawlee handles exponential backoff internally
});
```

## üì¶ Outputs

Crawler generates two types of outputs:

1. **JSON** (`./output/jobs.json`):
   - Complete data in structured format
   - Suitable for further processing

2. **CSV** (`./output/jobs.csv`):
   - Tabular format for analysis
   - Compatible with Excel/Google Sheets

## ü§ù Contributing

1. Fork the repository
2. Create a feature branch
3. Commit your changes
4. Push to the branch
5. Create a Pull Request

## üìÑ Licensing

This project uses dual licensing:

### Open-Source License (AGPL-3.0)
- Free use for non-commercial purposes
- Source code must remain open
- Derivative works must also be under AGPL-3.0
- Applies to network/API usage as well

### Commercial License
For commercial use, a commercial license is required, which provides:
- Ability to use code in closed projects
- No obligation to publish source code
- Priority support
- Custom modifications without sharing

To obtain a commercial license, contact:
- Email: [itssafer@itssafer.org](mailto:itssafer@itssafer.org)
- Subject: Job Scraper - Commercial License

## üôè Acknowledgments

- [Apify SDK](https://sdk.apify.com/)
- [Crawlee](https://crawlee.dev/)
- [Puppeteer](https://pptr.dev/)

## üë§ Author

- **Name**: Kristi√°n Ka≈°n√≠k
- **Email**: [itssafer@itssafer.org](mailto:itssafer@itssafer.org)
- **GitHub**: [ITSsafer-DevOps](https://github.com/ITSsafer-DevOps)

---

Created with ‚ù§Ô∏è for better job market analysis
